# Module_13_Challenge

By analyzing the success of over 34,000 organizations that have received funding from venture capital firm Alphabet Soup, we will create a binary classification model designed to predict whether a future applicant will become a successful business, and thus, will be worth funding. 

After importing the appropriate libraries, we begin by reading in the historical data provided for us by the company. Doing so reveals that much of the data is non-numerical, and that we have additional columns that are unnecessary for our analysis. By first dropping the columns that are not needed, we then identify and encode our categorical variables using OneHotEncoder. We can then create a new DataFrame with the encoded variables, and use the concat() function to combine the numerical values of our original DataFrame with the new encoded variables. At this point we may define our target and features from the concatenated DataFrame, split the features and target into training and testing datasets, and then scale the features data using StandardScaler(). With the data now cleaned and properly formated, we can create and compile our binary classification model. 

Using TensorFlow Keras, we created a two-layer deep neural network model, using the "relu" activation function on our hidden layers, and the "sigmoid" activation function on our output layer, as this is for binary classification. The number of nodes in the first hidden layer was based on the mean of the sum of the input and output layers, and the following layer used the mean of the sum of the previous layer and the output layer. The model was then compiled using the binary_crossentropy loss function, and then fit using the scaled features training data and the target training data. As per the instructions, we fit the model using 50 epochs. After fitting the model, we evaluated the model on loss and accuracy, and then saved the model as an HDF5 file. 

Having completed the original binary classification model, we then attempted to improve the model by defining two alternative neural networks, and then checking the accuracy against the original neural network. In the first alternative, we increased the number of nodes in the hidden layers. In the second alternative, we created more hidden layers and, consequently, more nodes overall. Doing so did not demonstrate a signficant change in our overall accuracy, though it is assumed that greater accuracy could be attained by generating more alternatives. Having completed 3 deep neural network models, all were saved in seperate HDF5 files, and stored in the Resources folder in the Starter_Code. 

